{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSfK3TzzOeBK"
   },
   "source": [
    "# Fundamentals of Data Science\n",
    "Winter Semester 2021\n",
    "\n",
    "## Prof. Fabio Galasso, Guido D'Amely, Alessandro Flaborea, Luca Franco, Muhammad Rameez Ur Rahman and Alessio Sampieri\n",
    "<galasso@di.uniroma1.it>, <damely@di.uniroma1.it>, <flaborea@di.uniroma1.it>, <franco@diag.uniroma1.it>, <rahman@di.uniroma1.it>, <alessiosampieri27@gmail.com>\n",
    "\n",
    "\n",
    "## Exercise 2: Classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UX84kDJ3uF3"
   },
   "source": [
    "In Exercise 2, you will re-derive and implement logistic regression and optimize the parameters with Gradient Descent and with the Newton's method. Also, in this exercise you will re-derive and implement Gassian Discriminant Analysis.\n",
    "We will use datasets generated from the make_classification function from the SkLearn library. Its first output contains the feature values $x^{(i)}_1$ and $x^{(i)}_2$ for the $i$-th data sample $x^{(i)}$. The second contains the ground truth label $y^{(i)}$ for each corresponding data sample.\n",
    "\n",
    "The completed exercise should be handed in as a single notebook file. Use Markdown to provide equations. Use the code sections to provide your scripts and the corresponding plots.\n",
    "Submit it by sending an email to galasso@di.uniroma1.it, flaborea@di.uniroma1.it, franco@diag.uniroma1.it and alessiosampieri27@gmail.com  by Wednesday November 17th 2021, 23:59."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_R7KkUpP3uF4"
   },
   "source": [
    "## Notation\n",
    "\n",
    "- $x^i$ is the $i^{th}$ feature vector\n",
    "- $y^i$ is the expected outcome for the $i^{th}$ training example\n",
    "- $m$ is the number of training examples\n",
    "- $n$ is the number of features\n",
    "\n",
    "Let's start by setting up our Python environment and importing the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "skGOzNBb3uF4"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np # imports a fast numerical programming library\n",
    "import scipy as sp # imports stats functions, amongst other things\n",
    "import matplotlib as mpl # this actually imports matplotlib\n",
    "import matplotlib.cm as cm # allows us easy access to colormaps\n",
    "import matplotlib.pyplot as plt # sets up plotting under plt\n",
    "import pandas as pd # lets us handle data as dataframes\n",
    "from sklearn.datasets import make_classification\n",
    "import seaborn as sns\n",
    "\n",
    "# sets up pandas table display\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "\n",
    "import seaborn as sns # sets up styles and gives us more plotting options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0G_LPstI3uF6"
   },
   "source": [
    "## Question 1: Logistic Regression with Gradient Ascent **(10 Points)**\n",
    "\n",
    "### Code and Theory \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffcP6hxn3uF7"
   },
   "source": [
    "#### Exercise 1.a **(3 Points)** Equations for the log likelihood, its gradient, and the gradient ascent update rule.\n",
    "\n",
    "Write and simplify the likelihood $L(\\theta)$ and log-likelihood $l(\\theta)$ of the parameters $\\theta$.\n",
    "\n",
    "Recall the probabilistic interpretation of the hypothesis $h_\\theta(x)= P(y=1|x;\\theta)$ and that $h_\\theta(x)=\\frac{1}{1+\\exp(-\\theta^T x)}$.\n",
    "\n",
    "Also derive the gradient $\\frac{\\delta l(\\theta)}{\\delta \\theta_j}$ of $l(\\theta)$ and write the gradient update equation. \n",
    "\n",
    "Question: Are we looking for a local minimum or a local maximum using the gradient ascent rule? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRMyk-Cj3uF7"
   },
   "source": [
    "################# Do not write above this line #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MIG1wwln3uF8"
   },
   "source": [
    "Assuming that $y$ could have only two different values $(0,1)$, the distribution of $y$ could be rapresented by a $Ber(h_{\\theta}(x))$, knowing that $m$ is the number of examples.\n",
    "\n",
    "We can write the $L(\\theta)$ as follow:\n",
    "\n",
    "$\n",
    "L(\\theta) =  \\mathbb{P}(\\vec{y} | x;\\theta) = \\prod_{i = 1}^{m}\\mathbb{P}(y^{(i)} | x^{(i)};\\theta) = \\prod_{i = 1}^{m}(h_{\\theta}(x^{(i)})^{y^{(i)}}\\times (1-(h_{\\theta}(x^{(i)}))^{1-y^{(i)}}\n",
    "$\n",
    "\n",
    "Then for $l(\\theta)$ we have:\n",
    "\n",
    "$\n",
    "l(\\theta) = \\log(L(\\theta)) = \\sum_{i=1}^{m}y^{(i)} \\log(h_{\\theta}(x^{(i)})) + (1-y^{(i)}) \\log (1-h_{\\theta}(x^{(i)})\n",
    "$\n",
    "\n",
    "Prob interp....\n",
    "\n",
    "The gradient $\\frac{\\delta l(\\theta)}{\\delta \\theta_j}$ of $l(\\theta)$ is:\n",
    "\n",
    "$\n",
    "\\frac{\\delta l(\\theta)}{\\delta \\theta_j} = \\sum_{i=1}^{m}(y^{(i)}-h_{\\theta}(x^{(i)}))\\times x_{j}^{(i)}\n",
    "$\n",
    "\n",
    "The gradient update equation is:\n",
    "\n",
    "$\n",
    "\\theta_{j} := \\theta_{j} + \\alpha \\sum_{i=1}^{m}(y^{(i)}-h_{\\theta}(x^{(i)}))\\times x_{j}^{(i)}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-VvNsMi3uF8"
   },
   "source": [
    "################# Do not write below this line #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbRCfu6u3uF9"
   },
   "source": [
    "#### Exercise 1.b **(7 Points)** Implementation of logistic regression with Gradient Ascent\n",
    "\n",
    "Code up the equations above to learn the logistic regression parameters. The dataset used here is created using the make_classification function present in the SkLearn library. $x^{(i)}_1$ and $x^{(i)}_2$ represent the two features for the $i$-th data sample $x^{(i)}$ and $y^{(i)}$ is its ground truth label.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NsEG77nM3uF9",
    "outputId": "9d5fe0ac-0c50-473f-cff2-bf976ac272ab"
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=500, n_features=2, n_informative=2, n_redundant=0, n_classes=2, random_state=5)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fkPYkGon3uF-",
    "outputId": "3be8c496-b37d-4944-e9b5-6ad4459e6ab3"
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x=X[:,0], y=X[:,1], hue=y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mE7FtQnL3uF-"
   },
   "source": [
    "Adding a column of 1's to $X$ to take into account the zero intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ukkMolWr3uF_"
   },
   "outputs": [],
   "source": [
    "x = np.hstack([np.ones((X.shape[0], 1)), X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M5XKPVYh3uF_",
    "outputId": "199ccb9c-2164-47ae-d1ef-56d8bba3e51c"
   },
   "outputs": [],
   "source": [
    "[x[:5,:],x[-5:,:]] # Plot the first and last 5 lines of x, now containing features x0 (constant=1), x1 and x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tA2GpW7D3uF_",
    "outputId": "f88f2e1d-6113-4b40-d92e-89354a7dc4e2"
   },
   "outputs": [],
   "source": [
    "[y[:5],y[-5:]] # Plot the first and last 5 lines of y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4N20uGxT3uGA"
   },
   "source": [
    "Define the sigmoid function \"sigmoid\", the function to compute the gradient of the log likelihood  \"grad_l\" and the gradient ascent algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atwd2qBN3uGA"
   },
   "source": [
    "################# Do not write above this line #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZhlHmIHI3uGA"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''\n",
    "    Function to compute the sigmoid of a given input x.\n",
    "    \n",
    "    Input:\n",
    "    x: it's the input data matrix. The shape is (N, H)\n",
    "\n",
    "    Output:\n",
    "    g: The sigmoid of the input x\n",
    "    '''\n",
    "    \n",
    "    g = 1 / (1 + np.exp(-x))\n",
    "\n",
    "    return g \n",
    "\n",
    "def log_likelihood(theta,features,target):\n",
    "    '''\n",
    "    Function to compute the log likehood of theta according to data x and label y\n",
    "    \n",
    "    Input:\n",
    "    theta: it's the model parameter matrix.\n",
    "    features: it's the input data matrix. The shape is (N, H)\n",
    "    target: the label array\n",
    "    \n",
    "    Output:\n",
    "    log_g: the log likehood of theta according to data x and label y\n",
    "    '''\n",
    "    \n",
    "    #####################################################\n",
    "    ##                 YOUR CODE HERE                  ##\n",
    "    #####################################################\n",
    "\n",
    "    h = sigmoid(np.sum(theta*features, axis = 1))\n",
    "    m = features.shape[0]\n",
    "\n",
    "    log_l = (1/m) * np.sum( (target*np.log(h) + (1-target) * np.log(1-h)))\n",
    "\n",
    "    return log_l\n",
    "\n",
    "\n",
    "def predictions(features, theta):\n",
    "    '''\n",
    "    Function to compute the predictions for the input features\n",
    "    \n",
    "    Input:\n",
    "    theta: it's the model parameter matrix.\n",
    "    features: it's the input data matrix. The shape is (N, H)\n",
    "    \n",
    "    Output:\n",
    "    preds: the predictions of the input features\n",
    "    '''\n",
    "    \n",
    "    #####################################################\n",
    "    ##                 YOUR CODE HERE                  ##\n",
    "    #####################################################\n",
    "\n",
    "    h = np.sum(theta*features, axis = 1)\n",
    "    preds = sigmoid(h) \n",
    "\n",
    "    return preds\n",
    "\n",
    "\n",
    "def update_theta(theta, target, preds, features, lr):\n",
    "    '''\n",
    "    Function to compute the gradient of the log likelihood\n",
    "    and then return the updated weights\n",
    "\n",
    "    Input:\n",
    "    theta: the model parameter matrix.\n",
    "    target: the label array\n",
    "    preds: the predictions of the input features\n",
    "    features: it's the input data matrix. The shape is (N, H)\n",
    "    lr: the learning rate\n",
    "    \n",
    "    Output:\n",
    "    theta: the updated model parameter matrix.\n",
    "    '''\n",
    "    \n",
    "    #####################################################\n",
    "    ##                 YOUR CODE HERE                  ##\n",
    "    #####################################################\n",
    "    \n",
    "    dim = features.shape  \n",
    "    m = dim[0] # number of training example\n",
    "    n = dim[1] # number of feature\n",
    "    \n",
    "    for j in range(n):\n",
    "        log_sum = 0\n",
    "        for i in range(m):\n",
    "            log_sum += ( target[i]-preds[i] )*features[i,j]\n",
    "        theta[j] = theta[j] + lr*log_sum\n",
    "    \n",
    "    return theta \n",
    "\n",
    "def gradient_ascent(theta, features, target, lr, num_steps):\n",
    "    '''\n",
    "    Function to execute the gradient ascent algorithm\n",
    "\n",
    "    Input:\n",
    "    theta: the model parameter matrix.\n",
    "    target: the label array\n",
    "    num_steps: the number of iterations \n",
    "    features: the input data matrix. The shape is (N, H)\n",
    "    lr: the learning rate\n",
    "    \n",
    "    Output:\n",
    "    theta: the final model parameter matrix.\n",
    "    log_likelihood_history: the values of the log likelihood during the process\n",
    "    '''\n",
    "\n",
    "    log_likelihood_history = np.zeros(num_steps)\n",
    "    \n",
    "    #####################################################\n",
    "    ##                 YOUR CODE HERE                  ##\n",
    "    #####################################################\n",
    "\n",
    "    for i in range(0,num_steps):\n",
    "        preds = predictions(features, theta)\n",
    "        theta = update_theta(theta, target, preds, features, lr)\n",
    "        log_likelihood_history[i] = log_likelihood(theta,features,target)\n",
    "\n",
    "    return theta, log_likelihood_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dExh39gt3uGA"
   },
   "source": [
    "################# Do not write below this line #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2q2DZXF3uGB"
   },
   "source": [
    "Check your grad_l implementation:\n",
    "grad_l applied to the theta_test (defined below) should provide a value for log_l_test close to the target_value (defined below); in other words the error_test should be 0, up to machine error precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h3UT5wav3uGB",
    "outputId": "808bbd3c-aedf-4ae6-8ccc-f407f2efd308"
   },
   "outputs": [],
   "source": [
    "target_value = -1.630501731599431\n",
    "\n",
    "output_test  = log_likelihood(np.array([-7,4,1]),x,y)\n",
    "error_test=np.abs(output_test-target_value)\n",
    "\n",
    "print(\"{:f}\".format(error_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55mr8J5d3uGB"
   },
   "source": [
    "Let's now apply the function gradient_ascent and print the final theta as well as theta_history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ajh8uvxR3uGB",
    "outputId": "4b06f084-d0d7-4867-f175-4f919606ddc3"
   },
   "outputs": [],
   "source": [
    "# Initialize theta0\n",
    "theta0 = np.zeros(x.shape[1])\n",
    "\n",
    "# Run Gradient Ascent method\n",
    "n_iter=1000\n",
    "theta_final, log_l_history = gradient_ascent(theta0,x,y,lr=0.01,num_steps=n_iter)\n",
    "print(theta_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MusdHuGZ3uGC"
   },
   "source": [
    "Let's plot the log likelihood over iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5BFYiF543uGC",
    "outputId": "a04eff45-8f56-4d69-d037-f208fd0430f9"
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(num=2)\n",
    "\n",
    "ax.set_ylabel('l(Theta)')\n",
    "ax.set_xlabel('Iterations')\n",
    "_=ax.plot(range(len(log_l_history)),log_l_history,'b.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYd890o33uGC"
   },
   "source": [
    "Plot the data and the decision boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0hXJewP13uGC",
    "outputId": "e3e8634e-045f-4273-b8e6-bfc5f93677c3"
   },
   "outputs": [],
   "source": [
    "# Generate vector to plot decision boundary\n",
    "x1_vec = np.linspace(X[:,0].min(),X[:,1].max(),2)\n",
    "\n",
    "# Plot raw data\n",
    "sns.scatterplot(x=X[:,0], y=X[:,1], hue=y, data=X)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.plot(x1_vec,(-x1_vec*theta_final[1]-theta_final[0])/theta_final[2], color=\"red\")\n",
    "plt.ylim(X[:,1].min()-1,X[:,1].max()+1)\n",
    "# Save the theta_final value for later comparisons\n",
    "theta_GA = theta_final.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsX3O_rG3uGC"
   },
   "source": [
    "################# Do not write above this line #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YyHpS-us3uGD"
   },
   "source": [
    "Discuss these two points:\n",
    "1. You have implemented the gradient ascent rule. Could we have also used gradient descent instead for the proposed problem? Why/Why not?\n",
    "2. Let's deeply analyze how the learning rate $\\alpha$ and the number of iterations affect the final results. Run the algorithm you have written for different values of $\\alpha$ and the number of iterations and look at the outputs you get. Is the decision boundary influenced by these parameters change? Why do you think these parameters are affecting/not affecting the results? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3hibN9m3uGD"
   },
   "source": [
    "################# Do not write below this line #################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbdZNYCl3uGD"
   },
   "source": [
    "## Question 2: Logistic Regression with non linear boundaries (7 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrcB4LXw3uGD"
   },
   "source": [
    "#### Exercise 2.a **(4 Points)** Polynomial features for logistic regression\n",
    "\n",
    "Define new features, e.g. of 2nd and 3rd degrees, and learn a logistic regression classifier by using the new features, by using the gradient ascent optimization algorithm you defined in Question 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSiX-ia_3uGD"
   },
   "source": [
    "In particular, we would consider a polynomial boundary with equation:\n",
    "\n",
    "$f(x_1, x_2) = c_0 + c_1 x_1 + c_2 x_2 + c_3 x_1^2 + c_4 x_2^2 + c_5 x_1 x_2 + c_6 x_1^3 + c_7 x_2^3 + c_8 x_1^2 x_2 + c_9 x_1 x_2^2$\n",
    "\n",
    "We would therefore compute 7 new features: 3 new ones for the quadratic terms and 4 new ones for the cubic terms.\n",
    "\n",
    "Create new arrays by stacking x and the new 7 features (in the order x1x1, x2x2, x1x2, x1x1x1, x2x2x2, x1x1x2, x1x2x2). In particular create x_new_quad by additionally stacking with x the quadratic features, and x_new_cubic by additionally stacking with x the quadratic and the cubic features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hXn0rvSM3uGD",
    "outputId": "7158a784-705f-4870-a591-01af0eca630d"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=500, n_features=2, n_informative=2, n_redundant=0, n_classes=2, random_state=5)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2X5n8Ohk3uGE"
   },
   "outputs": [],
   "source": [
    "x = np.hstack([np.ones((X.shape[0], 1)), X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OgY98L-13uGE",
    "outputId": "b4312579-1b84-4d3a-cd7b-fcc96662ca6a"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.scatterplot(x=X[:,0], y=X[:,1], hue=y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EanyUrtr3uGE",
    "outputId": "eb337d3c-7179-4114-988b-cb25025c6615"
   },
   "outputs": [],
   "source": [
    "# First extract features x1 and x2 from x and reshape them to x1 vector arrays\n",
    "x1 = x[:,1]\n",
    "x2 = x[:,2]\n",
    "x1 = x1.reshape(x1.shape[0], 1)\n",
    "x2 = x2.reshape(x2.shape[0], 1)\n",
    "print(x[:5,:]) # For visualization of the first 5 values\n",
    "print(x1[:5,:]) # For visualization of the first 5 values\n",
    "print(x2[:5,:]) # For visualization of the first 5 values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1dL4LTvN3uGF"
   },
   "source": [
    "################# Do not write above this line #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nv_yctWU3uGF"
   },
   "source": [
    "Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zicVdhc73uGF"
   },
   "outputs": [],
   "source": [
    "\n",
    "def new_features(x, degree=2):\n",
    "    '''\n",
    "    Function to create n-degree features from the input \n",
    "\n",
    "    Input:\n",
    "    x: the initial features\n",
    "    degree: the maximum degree you want the features\n",
    "    \n",
    "    Output:\n",
    "    features: the final features. \n",
    "              2nd degree features must have the order [x, x1x1, x1x2, x2x2]\n",
    "              3nd degree features must have the order [x, x1x1, x1x2, x2x2, x1x1x1, x1x1x2, x1x2x2, x2x2x2]\n",
    "    '''\n",
    "    \n",
    "    # Initialize features as x0\n",
    "    features = np.ones(x[:,1].shape[0])\n",
    "    \n",
    "    x0 = x[:,0]\n",
    "    x1 = x[:,1]\n",
    "    x2 = x[:,2]\n",
    "    x1x2 = x[:,1:3]\n",
    "    \n",
    "    generating_matrix = np.empty((x1x2.shape[0], x1x2.shape[1]))\n",
    "    \n",
    "    # Generating the new_features\n",
    "    if degree < 2:\n",
    "        return x\n",
    "    else:\n",
    "        for i in range(1, degree):\n",
    "            array_1 = np.empty((generating_matrix.shape[0], generating_matrix.shape[1]))\n",
    "            x1 = x1.T\n",
    "            np.multiply(generating_matrix, x1[:,np.newaxis], out=array_1)\n",
    "            print(array_1)\n",
    "\n",
    "            array_2 = np.empty((generating_matrix.shape[0], generating_matrix.shape[1]))\n",
    "            x2 = x2.T\n",
    "            np.multiply(generating_matrix, x2[:,np.newaxis] ,out=array_2)\n",
    "            print(array_2)\n",
    "\n",
    "            # Update the matrix that generates new features with products with [x1] and [x2]\n",
    "            new_generated = np.column_stack((array_1, array_2[:,-1]))\n",
    "            print(new_generated)\n",
    "            generating_matrix = np.column_stack((generating_matrix, new_generated))\n",
    "            print(generating_matrix)\n",
    "\n",
    "        # In the end concatenate to result array last feature x0 (expressed as column vector)\n",
    "        features = np.empty((x.shape[0], np.math.factorial(degree) + x.shape[1]))\n",
    "        features = np.column_stack((x0.reshape(1, len(x0)).T, generating_matrix))\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.array([[1,2,3], [4,5,6], [6,8,9]])\n",
    "features = new_features(xx, degree=3)\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vzyJ450Z3uGF"
   },
   "source": [
    "################# Do not write below this line #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D7pukmkA3uGF"
   },
   "outputs": [],
   "source": [
    "x_new_quad = new_features(x, degree=2)\n",
    "x_new_cubic = new_features(x, degree=3)\n",
    "\n",
    "#reordering output features\n",
    "temp = np.copy(x_new_quad[:, -1])\n",
    "x_new_quad[:, -1] = x_new_quad[:, -2]\n",
    "x_new_quad[:, -2] = temp\n",
    "\n",
    "temp = np.copy(x_new_cubic[:, -1])\n",
    "x_new_cubic[:, -1] = x_new_cubic[:, -2]\n",
    "x_new_cubic[:, -2] = x_new_cubic[:, -3]\n",
    "x_new_cubic[:, -3] = temp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFlwv5JY3uGF"
   },
   "source": [
    "Now use the gradient ascent optimization algorithm to learn theta by maximizing the log-likelihood, both for the case of x_new_quad and x_new_cubic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jFTkk32y3uGF",
    "outputId": "e1b108a0-a1c1-4f6a-9c5a-a8691e97000c"
   },
   "outputs": [],
   "source": [
    "# Initialize theta0, in case of quadratic features\n",
    "theta0_quad = np.zeros(x_new_quad.shape[1])\n",
    "\n",
    "theta_final_quad, log_l_history_quad = gradient_ascent(theta0_quad,x_new_quad,y,lr=0.5,num_steps=n_iter)\n",
    "\n",
    "# Initialize theta0, in case of quadratic and cubic features\n",
    "theta0_cubic = np.zeros(x_new_cubic.shape[1])\n",
    "\n",
    "# Run Newton's method, in case of quadratic and cubic features\n",
    "theta_final_cubic, log_l_history_cubic = gradient_ascent(theta0_cubic,x_new_cubic,y,lr=0.5,num_steps=n_iter)\n",
    "\n",
    "# check and compare with previous results\n",
    "print(theta_final_quad)\n",
    "print(theta_final_cubic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dxIgGmBD3uGG",
    "outputId": "dec306a2-6170-434a-f543-cbbd2dc7f3fd"
   },
   "outputs": [],
   "source": [
    "# Plot the log likelihood values in the optimization iterations, in one of the two cases.\n",
    "fig,ax = plt.subplots(num=2)\n",
    "\n",
    "ax.set_ylabel('l(Theta)')\n",
    "ax.set_xlabel('Iterations')\n",
    "_=ax.plot(range(len(log_l_history_quad)),log_l_history_quad,'b.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1r8rLBE3uGG"
   },
   "source": [
    "#### Exercise 2.b **(3 Points)** Plot the computed non-linear boundary and discuss the questions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zy_2fRVP3uGG"
   },
   "source": [
    "First, define a boundary_function to compute the boundary equation for the input feature vectors $x_1$ and $x_2$, according to estimated parameters theta, both in the case of quadratic (theta_final_quad) and of quadratic and cubic features (theta_final_cubic). Refer for the equation to the introductory part of Question 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MU7WQQxe3uGG"
   },
   "source": [
    "################# Do not write above this line #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v67og3I73uGG"
   },
   "source": [
    "Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fd4r2Z3z3uGG"
   },
   "outputs": [],
   "source": [
    "def boundary_function(x1_vec, x2_vec, theta_final):\n",
    "    \n",
    "    x1_vec, x2_vec = np.meshgrid(x1_vec,x2_vec)\n",
    "    \n",
    "    if len(theta_final) == 6:\n",
    "        # boundary function value for features up to quadratic\n",
    "        c_0, c_1, c_2, c_3, c_4, c_5 = theta_final\n",
    "        # f =\n",
    "    elif len(theta_final) == 10:\n",
    "        # boundary function value for features up to cubic\n",
    "        c_0, c_1, c_2, c_3, c_4, c_5, c_6, c_7, c_8, c_9 = theta_final\n",
    "        # f = \n",
    "    else:\n",
    "        raise(\"Number of Parameters is not correct\")\n",
    "        \n",
    "    return x1_vec, x2_vec, f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2udd0d63uGG"
   },
   "source": [
    "################# Do not write below this line #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIvgwCTL3uGH"
   },
   "source": [
    "Now plot the decision boundaries corresponding to the theta_final_quad and theta_final_cubic solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6SSIfi6r3uGH",
    "outputId": "2bba76b1-38c3-4e3b-d2c2-80d973585da6"
   },
   "outputs": [],
   "source": [
    "x1_vec = np.linspace(X[:,0].min()-1,X[:,0].max()+1,200);\n",
    "x2_vec = np.linspace(X[:,1].min()-1,X[:,1].max()+1,200);\n",
    "\n",
    "x1_vec, x2_vec, f = boundary_function(x1_vec, x2_vec, theta_final_quad)\n",
    "\n",
    "sns.scatterplot(x=X[:,0], y=X[:,1], hue=y, data=X);\n",
    "\n",
    "plt.contour(x1_vec, x2_vec, f, colors=\"red\", levels=[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TY5-ve1N3uGH",
    "outputId": "57da3c01-5433-42f4-e783-7bcdb7b79148"
   },
   "outputs": [],
   "source": [
    "x1_vec = np.linspace(X[:,0].min()-1,X[:,0].max()+1,200);\n",
    "x2_vec = np.linspace(X[:,1].min()-1,X[:,1].max()+1,200);\n",
    "\n",
    "x1_vec, x2_vec, f = boundary_function(x1_vec, x2_vec, theta_final_cubic)\n",
    "\n",
    "sns.scatterplot(x=X[:,0], y=X[:,1], hue=y, data=X);\n",
    "\n",
    "plt.contour(x1_vec, x2_vec, f, colors=\"red\", levels=[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLas_a4tUpdj"
   },
   "source": [
    "#### Confusion Matrix\n",
    "\n",
    "Here you can see the confusion matrices related to the three models you've implemented. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yypj8CpgUpdj"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l-7oSjbzUpdk",
    "outputId": "0049f177-9da1-479f-ff03-976f7b52bbec"
   },
   "outputs": [],
   "source": [
    "## logistic regression with linear buondary\n",
    "\n",
    "z = np.dot(x,theta_final)\n",
    "probabilities = sigmoid(z)\n",
    "y_hat = np.array(list(map(lambda x: 1 if x>0.5 else 0, probabilities)))\n",
    "confusion_matrix(y, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zuac0wuhUpdk",
    "outputId": "5252c2ed-fe81-4162-a56f-39cf36762526"
   },
   "outputs": [],
   "source": [
    "## logistic regression with non linear buondary - quadratic\n",
    "\n",
    "z = np.dot(x_new_quad,theta_final_quad)\n",
    "probabilities = sigmoid(z)\n",
    "y_hat = np.array(list(map(lambda x: 1 if x>0.5 else 0, probabilities)))\n",
    "confusion_matrix(y, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GfYPFswWUpdk",
    "outputId": "ce659e3b-a205-4c45-b2ee-73adfdc489eb"
   },
   "outputs": [],
   "source": [
    "## logistic regression with non linear buondary - cubic\n",
    "\n",
    "z = np.dot(x_new_cubic,theta_final_cubic)\n",
    "probabilities = sigmoid(z)\n",
    "y_hat = np.array(list(map(lambda x: 1 if x>0.5 else 0, probabilities)))\n",
    "confusion_matrix(y, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Crc3nn633uGH"
   },
   "source": [
    "################# Do not write above this line #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "do098TOZ3uGH"
   },
   "source": [
    "Write now your considerations. Discuss in particular:\n",
    "- Look back at the plots you have generated. What can you say about the differences between the linear, quadratic, and cubic decision boundaries? Can you say if the model is improving in performances, increasing the degree of the polynomial? Do you think you can incur in underfitting increasing more and more the degree?\n",
    "- Let's now delve into some quantitative analysis. The three tables you have generated represent the confusion matrix for the model you have implemented in the first two questions. What can you say about actual performances? Does the increase of the degree have a high effect on the results? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8ZPI0wn3uGH"
   },
   "source": [
    "################# Do not write below this line #################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTeuyG-S3uGH"
   },
   "source": [
    "## Question 3: Multinomial Classification (Softmax Regression) **(13 Points)**\n",
    "\n",
    "### Code and Theory **(10 Points)**\n",
    "### Report **(3 Points)**\n",
    "\n",
    "#### Exercise 3.a **(4 Points)**\n",
    "\n",
    "In the multinomial classification we generally have $K>2$ classes. So the label for the $i$-th sample $X_i$ is $y_i\\in\\{1,...,K\\}$, where $i=1,...,N$. The output class for each sample is estimated by returning a score $s_i$ for each of the K classes. This results in a vector of scores of dimension K. \n",
    "In this exercise we'll use the *Softmax Regression* model, which is the natural extension of *Logistic Regression* for the case of more than 2 classes. The score array is given by the linear model:\n",
    "\n",
    "\\begin{equation}\n",
    "s_i =  X_i \\theta\n",
    "\\end{equation}\n",
    "\n",
    "Scores may be interpreted probabilistically, upon application of the function *softmax*. The position in the vector with the highest probability will be predicted as the output class. The probability of the class k for the $i$-th data sample is:\n",
    "\n",
    "\\begin{equation}\n",
    "p_{ik} = \\frac{\\exp(X_i \\theta_k)}{\\sum_{j=1}^K(X_i \\theta_j))}\n",
    "\\end{equation}\n",
    "\n",
    "We will adopt the *Cross Entropy* loss and optimize the model via *Gradient Descent*. \n",
    "In the first of this exercise we have to: \n",
    "-    Write the equations of the Cross Entropy loss for the Softmax regression model;\n",
    "-    Compute the equation for the gradient of the Cross Entropy loss for the model, in order to use it in the gradient descent algorithm.\n",
    "\n",
    "#### A bit of notation\n",
    "\n",
    "*  N: is the number of samples \n",
    "*  K: is the number of classes\n",
    "*  X: is the input dataset and it has shape (N, H) where H is the number of features\n",
    "*  y: is the output array with the labels; it has shape (N, 1)\n",
    "*  $\\theta$: is the parameter matrix of the model; it has shape (H, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHX1s7jp3uGI"
   },
   "source": [
    "################# Do not write above this line #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixObV4w43uGI"
   },
   "source": [
    "Your equations here.\n",
    "\n",
    "\\begin{equation}\n",
    "L(\\theta) = ...\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{\\theta_k} L(\\theta) = ...\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZfeKXUs3uGI"
   },
   "source": [
    "################# Do not write below this line #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMxrcWc53uGI"
   },
   "source": [
    "#### Exercise 3.b **(4 Points)**\n",
    "\n",
    "Now we will implement the code for the equations. Let's implement the functions:\n",
    "-  softmax \n",
    "-  CELoss\n",
    "-  CELoss gradient\n",
    "-  gradient descent\n",
    "\n",
    "We generate a toy dataset with *sklearn* library. Do not change anything outside the parts provided of your own code (else the provided checkpoint will not work)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wJJ-kcEq3uGI",
    "outputId": "ab18fe5e-9880-4c13-8538-23fc8545c589"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=300, n_features=7, n_informative=7, n_redundant=0, n_classes=3, random_state=1)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RrCmafP3uGI"
   },
   "source": [
    "As a hint for the implementations of your functions: consider the labels $y$ as one-hot vector. This will allow matrix operations (element-wise multiplication and summation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQaSqENw3uGI"
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "def class2OneHot(vec):\n",
    "    out_sparse = scipy.sparse.csr_matrix((np.ones(vec.shape[0]), (vec, np.array(range(vec.shape[0])))))\n",
    "    out_onehot = np.array(out_sparse.todense()).T\n",
    "    return out_onehot\n",
    "\n",
    "y_onehot = class2OneHot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kl6Vnzyh3uGJ"
   },
   "source": [
    "Let's visualize the generated dataset. We use as visualizzation method the *Principal Component Analysis* (PCA). PCA summarize the high-dimensional feature vectors of each sample into 2  features, which we can illustrate with a 2D plot. Look at the following plot, the 3 generated classes do not seem separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c6y1_Uj83uGJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(X)\n",
    "principalDf = pd.DataFrame(data = principalComponents, columns = ['pc1', 'pc2'])\n",
    "finalDf = pd.concat([principalDf, pd.DataFrame(y, columns = ['target'])], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ZiQzwCr3uGJ",
    "outputId": "712c6d1a-e97b-47c0-bd1b-dc3ffb4ea2ab"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.scatterplot(x='pc1', y='pc2', hue='target', data=finalDf);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ac03aJju3uGJ"
   },
   "source": [
    "################# Do not write above this line #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i9rZrYaw3uGJ"
   },
   "outputs": [],
   "source": [
    "def softmax(theta, X):\n",
    "    '''\n",
    "    Function to compute associated probability for each sample and each class.\n",
    "    \n",
    "    Input:\n",
    "    theta: it's the model parameter matrix. The shape is (H, K)\n",
    "    X: it's the input data matrix. The shape is (N, H)\n",
    "\n",
    "    Output:\n",
    "    softmax: it's the matrix containing probability for each sample and each class. The shape is (N, K)\n",
    "    '''\n",
    "    \n",
    "    #####################################################\n",
    "    ##                 YOUR CODE HERE                  ##\n",
    "    #####################################################\n",
    "    \n",
    "    return softmax\n",
    "\n",
    "\n",
    "def CELoss(theta, X, y_onehot):\n",
    "    '''\n",
    "    Function to compute softmax regression model and Cross Entropy loss.\n",
    "    \n",
    "    Input:\n",
    "    theta: it's the model parameter matrix. The shape is (H, K)\n",
    "    X: it's the input data matrix. The shape is (N, H)\n",
    "    y_onehot: it's the label array in encoded as one hot vector. The shape is (N, K)\n",
    "\n",
    "    Output:\n",
    "    loss: The scalar that is the mean error for each sample.\n",
    "    '''\n",
    "    \n",
    "    #####################################################\n",
    "    ##                 YOUR CODE HERE                  ##\n",
    "    #####################################################\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def CELoss_jacobian(theta, X, y_onehot):\n",
    "    '''\n",
    "    Function to compute gradient of the cross entropy loss with respect the parameters.\n",
    "    \n",
    "    Input:\n",
    "    theta: it's the model parameter matrix. The shape is (H, K)\n",
    "    X: it's the input data matrix. The shape is (N, H)\n",
    "    y_onehot: it's the label array in encoded as one hot vector. The shape is (N, K)\n",
    "\n",
    "    Output:\n",
    "    jacobian: A matrix with the partial derivatives of the loss. The shape is (H, K)\n",
    "    '''\n",
    "    \n",
    "    #####################################################\n",
    "    ##                 YOUR CODE HERE                  ##\n",
    "    #####################################################\n",
    "    \n",
    "    return jacobian\n",
    "\n",
    "\n",
    "def gradient_descent(theta, X, y_onehot, alpha=0.01, iterations=100):\n",
    "    '''\n",
    "    Function to compute gradient of the cross entropy loss with respect the parameters.\n",
    "    \n",
    "    Input:\n",
    "    theta: it's the model parameter matrix. The shape is (H, K)\n",
    "    X: it's the input data matrix. The shape is (N, H)\n",
    "    y_onehot: it's the label array in encoded as one hot vector. The shape is (N, K)\n",
    "    alpha: it's the learning rate, so it determines the speed of each step of the GD algorithm\n",
    "    iterations: it's the total number of step the algorithm performs\n",
    "\n",
    "    Output:\n",
    "    theta: it's the updated matrix of the parameters after all the iterations of the optimization algorithm. The shape is (H, K)\n",
    "    loss_history: it's an array with the computed loss after each iteration\n",
    "    '''\n",
    "\n",
    "    # We initialize an empty array to be filled with loss value after each iteration\n",
    "    loss_history = np.zeros(iterations)\n",
    "    \n",
    "    # With a for loop we compute the steps of GD algo\n",
    "    for it in range(iterations):\n",
    "        \n",
    "        #####################################################\n",
    "        ##                 YOUR CODE HERE                  ##\n",
    "        #####################################################\n",
    "        \n",
    "    return theta, loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHEWCMjo3uGJ"
   },
   "source": [
    "################# Do not write below this line #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kZPZLWLz3uGJ"
   },
   "outputs": [],
   "source": [
    "# Initialize a theta matrix with random parameters\n",
    "theta0 = np.random.rand(X.shape[1], len(np.unique(y)))\n",
    "\n",
    "print(\"Initial Loss with initialized theta is:\", CELoss(theta0, X, y_onehot))\n",
    "\n",
    "# Run Gradient Descent method\n",
    "n_iter = 1000\n",
    "theta_final, log_l_history = gradient_descent(theta0, X, y_onehot, alpha=0.01, iterations=n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ejE4e-kr3uGJ"
   },
   "outputs": [],
   "source": [
    "theta_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "81bEfMul3uGK"
   },
   "outputs": [],
   "source": [
    "loss = CELoss(theta_final, X, y_onehot)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BKcoJLSP3uGK"
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(num=2)\n",
    "\n",
    "ax.set_ylabel('loss')\n",
    "ax.set_xlabel('Iterations')\n",
    "_=ax.plot(range(len(log_l_history)), log_l_history,'b.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7Ey8deA3uGK"
   },
   "source": [
    "#### Exercise 3.c **(2 Points)**\n",
    "\n",
    "Let's now evaluate the goodness of the learnt based on accuracy:\n",
    "\n",
    "\\begin{equation}\n",
    "Accuracy = \\frac{Number\\ of\\ correct\\ predictions}{Total\\ number\\ of\\ predictions}\n",
    "\\end{equation}\n",
    "\n",
    "Implement the compute_accuracy function. You may compare the accuracy achieved with learnt model Vs. a random model (random $\\Theta$) or one based on $\\Theta$'s filled with zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEOB44vk3uGK"
   },
   "source": [
    "################# Do not write above this line #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rh-cy3XX3uGK"
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(theta, X, y):\n",
    "    '''\n",
    "    Function to compute accuracy metrics of the softmax regression model.\n",
    "    \n",
    "    Input:\n",
    "    theta: it's the final parameter matrix. The one we learned after all the iterations of the GD algorithm. The shape is (H, K)\n",
    "    X: it's the input data matrix. The shape is (N, H)\n",
    "    y: it's the label array. The shape is (N, 1)\n",
    "\n",
    "    Output:\n",
    "    accuracy: Score of the accuracy.\n",
    "    '''\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EjZMkNYF3uGK"
   },
   "source": [
    "################# Do not write below this line #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1-OxfPQ3uGK"
   },
   "outputs": [],
   "source": [
    "compute_accuracy(theta_final, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0 = np.random.rand(X.shape[1], len(np.unique(y)))\n",
    "compute_accuracy(theta0, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rF1eBJjs3uGK"
   },
   "outputs": [],
   "source": [
    "compute_accuracy(np.zeros((X.shape[1], len(np.unique(y)))), X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vu2tNIpT3uGL"
   },
   "source": [
    "### Report **(3 Points)**\n",
    "\n",
    "Experiment with different values for the learning rate $\\alpha$ and the number of iterations. Look how the loss plot changes the convergence rate and the resulting accuracy metric. Report also execution time of each run. For this last step you could you %%time at the beginning of the cell to display time needed for the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FeeMj9BB3uGL"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Initialize a theta matrix with random parameters\n",
    "theta0 = np.random.rand(X.shape[1], len(np.unique(y)))\n",
    "\n",
    "print(\"Initial Loss with initialized theta is:\", CELoss(theta0, X, y_onehot))\n",
    "\n",
    "# Run Gradient Descent method\n",
    "n_iter = 100\n",
    "theta_final, log_l_history = gradient_descent(theta0, X, y_onehot, alpha=0.001, iterations=n_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NAW9sLt83uGL"
   },
   "source": [
    "**Write your Report here**\n",
    "\n",
    "\n",
    "| LR | Iter | Accuracy | Time |\n",
    "|---|---|---|---|\n",
    "|  |  |  |  |\n",
    "|  |  |  |  |\n",
    "|  |  |  |  |\n",
    "| ... |  |  |  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VyTr567oXqSb"
   },
   "source": [
    "## Question 4: Multinomial Naive Bayes **(6 Points)**\n",
    "\n",
    "### Code and Theory\n",
    "\n",
    "The Naive Bayes classifier is a probabilistic machine learning model often used for classification tasks, e.g. document classification problems.\n",
    "\n",
    "In the multinomial Naive Bayes classification you generally have $K>2$ classes, and the features are assumed to be generated from a multinomial distribution.\n",
    "\n",
    "##### __*Example Data*__\n",
    "General models consider input data as values. In the case of MultinomialNB, being used mainly in the field of document classification, these data consider how many features $X_i$ are present in the sample. Basically, it is a count of features within each document.\n",
    "\n",
    "Taking into account $D=3$ documents and a vocabulary consisting of $N=4$ words, the data are considered as follows.\n",
    "\n",
    "|  | $w_1$ | $w_2$ | $w_3$ | $w_4$ |\n",
    "|---|---|---|---|---|\n",
    "| $d_1$  | 3 | 0 | 1 | 1 |\n",
    "| $d_2$ | 2 | 1 | 3 |0|\n",
    "| $d_3$ | 2 | 2 | 0 |2|\n",
    "\n",
    "By randomly generating the class to which each document belongs we have $y=[1,0,1]$\n",
    "\n",
    "\n",
    "\n",
    "##### __*A bit of notation*__\n",
    "- $Y =\\{y_1, y_2, ... , y_{|Y|}\\}$: set of classes\n",
    "\n",
    "- $V =\\{w_1, w_2, ... , w_{|V|}\\}$: set of vocabulary\n",
    "\n",
    "-  $D =\\{d_1, d_2, ... , d_{|D|}\\}$: set of documents \n",
    "\n",
    "-  $N_{yi}$:  count of a specific word $w_i$ in each unique class, e.g. for $y=1$ you select $D_1$ and $D_3$, then for third column  you have $N_{y,3}=1$ \n",
    "-  $N_y$: total count of features for a specific class, e.g. for $y=1$ you sum all rows values which the correspondent label is 1, so $N_y=11$ \n",
    "\n",
    "-  $n$: total number of features (words in vocabulary)\n",
    "-  $\\alpha$: smoothing parameters\n",
    "\n",
    "##### __*Task*__\n",
    "Find the class $y$ to which the document is most likely to belong given the words $w$.\n",
    "Use the Bayes formula and the posterior probability for it.\n",
    "\n",
    "Bayes Formula:\n",
    "\\begin{equation}\n",
    "P(A|B) = \\frac{P(A)*P(B|A)}{P(B)}\n",
    "\\end{equation}\n",
    "\n",
    "Where:\n",
    "- P(A): Prior probability of A\n",
    "- P(B): Prior probability of B\n",
    "- P(B|A): Likelihood, multiplying posterior probability, that is multinomial Naive Bayes is:\n",
    "\\begin{equation}\n",
    "P(B|A) = \\left(\\frac{N_{yi}+\\alpha}{N_{y}+\\alpha*n\\_features}\\right)^{X_{doc,i}}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYVNUVZhVSn2"
   },
   "source": [
    "**Reminder: do not change any part of this notebook outside the assigned work spaces**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "excFpvHG8uwO"
   },
   "source": [
    "#### Generate random dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rtE4BJAa82E1",
    "outputId": "9c0bd394-97c9-4569-c56e-bc39a71aa11d"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = make_classification(n_samples=300, n_features=7, n_informative=7, n_redundant=0, n_classes=3, random_state=1)\n",
    "X = np.floor(X)-np.min(np.floor(X))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OT-css9uA9Px"
   },
   "source": [
    "#### Step0: $N_y$ and $N_{yi}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ymmh4GDnBAQm"
   },
   "outputs": [],
   "source": [
    "def feature_count(X, y, classes, n_classes, n_features):\n",
    "        '''\n",
    "        Function to compute the count of a specific word in each unique class and the total count.\n",
    "        \n",
    "        Input:\n",
    "        X: it's the input data matrix.\n",
    "        y: label array\n",
    "        classes: unique values of y\n",
    "        n_classes: number of classes\n",
    "        n_features: it's the number of word in Vocabulary.\n",
    "\n",
    "        Output:\n",
    "        N_yi:   count of a specific word $w_i$ in each unique class\n",
    "        N_y: total count of features for a specific class\n",
    "        '''\n",
    "        N_yi = np.zeros((n_classes, n_features)) # feature count\n",
    "        N_y = np.zeros((n_classes)) # total count \n",
    "\n",
    "        #####################################################\n",
    "        ##                 YOUR CODE HERE                  ##\n",
    "        #####################################################\n",
    "        \n",
    "        return N_yi, N_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TA5mO9khBwaR"
   },
   "outputs": [],
   "source": [
    "n_samples_train, n_features = X_train.shape\n",
    "classes = np.unique(y_train)\n",
    "n_classes = 3\n",
    "alpha = 0.1\n",
    "\n",
    "N_yi, N_y = feature_count(X_train, y_train, classes, n_classes, n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5ht6n-36Y6L"
   },
   "source": [
    "#### Step1: Prior Probability\n",
    "The probability of a document being in a specific category from the given set of documents.\n",
    "\n",
    "######################################\n",
    "\n",
    "Your equations here:\n",
    "\\begin{equation}\n",
    "P(y_j) = ...\n",
    "\\end{equation}\n",
    "\n",
    "######################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Fftcowl8h5Y"
   },
   "outputs": [],
   "source": [
    "def prior_(X, y, n_classes, n_samples):\n",
    "        \"\"\"\n",
    "        Calculates prior for each unique class in y.\n",
    "\n",
    "        Input:\n",
    "        X: it's the input data matrix.\n",
    "        y: label array\n",
    "        n_classes: number of classes\n",
    "        n_samples: number of documents\n",
    "\n",
    "        Output:\n",
    "        P: prior probability for each class. Shape: (, n_classes)\n",
    "        \"\"\"\n",
    "        classes = np.unique(y)\n",
    "        P = np.zeros(n_classes)\n",
    "\n",
    "        # Implement Prior Probability P(A)\n",
    "        #####################################################\n",
    "        ##                 YOUR CODE HERE                  ##\n",
    "        #####################################################\n",
    "\n",
    "        return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-N1eIw6N8lPN",
    "outputId": "3aa4f584-df74-42be-c7eb-b71a3563a51a"
   },
   "outputs": [],
   "source": [
    "prior_prob = prior_(X_train, y_train, n_classes, n_samples_train)\n",
    "print(prior_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOjKoCJS9Kx7"
   },
   "source": [
    "#### Step2\n",
    "Posterior Probability: The conditional probability of a word occurring in a document given that the document belongs to a particular category.\n",
    "\n",
    "\\begin{equation}\n",
    "P(w_i|y_j) = \\left(\\frac{N_{yi}+\\alpha}{N_{y}+\\alpha*n\\_features}\\right)^{X_{doc,i}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Likelihood for a single document: \n",
    "######################################\n",
    "\n",
    "Your equations here:\n",
    "\\begin{equation}\n",
    "P(w|y_j) = ...\n",
    "\\end{equation}\n",
    "\n",
    "######################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8KF89419_fXo"
   },
   "outputs": [],
   "source": [
    "def posterior_(x_i, i, h, N_y, N_yi, n_features, alpha):\n",
    "        \"\"\"\n",
    "        Calculates posterior probability. aka P(w_i|y_j) using equation in the notebook.\n",
    "        \n",
    "        Input:\n",
    "        x_i: feature x_i\n",
    "        i: feature index.  \n",
    "        h: a class in y\n",
    "        N_yi:   count of a specific word in each unique class\n",
    "        N_y: total count of features for a specific class\n",
    "        n_features: it's the number of word in Vocabulary.\n",
    "        alpha: smoothing parameter\n",
    "\n",
    "        Output:\n",
    "        posterior: P(xi | y). Float.\n",
    "        \"\"\"\n",
    "\n",
    "        # Implement Posterior Probability\n",
    "        #####################################################\n",
    "        ##                 YOUR CODE HERE                  ##\n",
    "        #####################################################\n",
    "        return posterior\n",
    "    \n",
    "def likelihood_(x, h, N_y, N_yi, n_features, alpha):\n",
    "        \"\"\"\n",
    "        Calculates Likelihood P(w|j_i).\n",
    "        \n",
    "        Input:\n",
    "        x: a row of test data. Shape(n_features,)\n",
    "        h: a class in y\n",
    "        N_yi:   count of a specific word in each unique class\n",
    "        N_y: total count of features for a specific class\n",
    "        n_features: it's the number of word in Vocabulary.\n",
    "        alpha: smoothing parameter\n",
    "\n",
    "        Output:\n",
    "        likelihood: Float.\n",
    "        \"\"\"\n",
    "        \n",
    "        tmp = []\n",
    "        for i in range(x.shape[0]):\n",
    "            tmp.append(posterior_(x[i], i, h, N_y, N_yi, n_features, alpha))\n",
    "\n",
    "        # Implement Likelihood\n",
    "        #####################################################\n",
    "        ##                 YOUR CODE HERE                  ##\n",
    "        #####################################################\n",
    "        \n",
    "        return likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zX90btoiFfJp",
    "outputId": "1ca5cab8-a5d4-461e-8f78-124c01980311"
   },
   "outputs": [],
   "source": [
    "# Example of likelihood for first document\n",
    "likelihood_(X_test[0], 0, N_y, N_yi, n_features, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pXOqMwGLGn6"
   },
   "source": [
    "#### Step3\n",
    "Joint Likelihood that, given the words, the documents belongs to specific class\n",
    "######################################\n",
    "\n",
    "Your equations here:\n",
    "\\begin{equation}\n",
    "P(y_i|w) = ...\n",
    "\\end{equation}\n",
    "\n",
    "######################################\n",
    "\n",
    "Finally, from the probability that the document is in that class given the words, take the argument correspond to max value.\n",
    "\n",
    "\\begin{equation}\n",
    "y(D) = argmax_{y \\in Y} \\frac{P(y|w)}{\\sum_{j}P(y_j|w)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HcgDILSjLFXS"
   },
   "outputs": [],
   "source": [
    "def joint_likelihood(X, prior_prob, classes, n_classes, N_y, N_yi, n_features, alpha):\n",
    "        \"\"\"\n",
    "        Calculates the joint probability P(y_i|w) for each class and makes it probability.\n",
    "        Then take the argmax.\n",
    "        \n",
    "        Input:\n",
    "        X: test data\n",
    "        prior_prob:\n",
    "        classes:\n",
    "        n_classes:\n",
    "        N_yi:   count of a specific word in each unique class\n",
    "        N_y: total count of features for a specific class\n",
    "        n_features: it's the number of word in Vocabulary.\n",
    "        alpha: smoothing parameter\n",
    "\n",
    "        Output:\n",
    "        predicted_class: Predicted class of the documents. Int. Shape: (,#documents)\n",
    "        \"\"\"\n",
    "        samples, features = X.shape\n",
    "        predict_proba = np.zeros((samples,n_classes)) \n",
    "        \n",
    "        # Calculate Joint Likelihood of each row for each class, then normalize in order to make them probabilities\n",
    "        # Finally take the argmax to have the predicted class for each document\n",
    "        #####################################################\n",
    "        ##                 YOUR CODE HERE                  ##\n",
    "        #####################################################\n",
    "        \n",
    "        return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQxDCQqPL_mq"
   },
   "outputs": [],
   "source": [
    "yhat = joint_likelihood(X_test, prior_prob, classes, n_classes, N_y, N_yi, n_features, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-92CnnKzDdT"
   },
   "source": [
    "#### Step4: Calculate the Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o7N-sS-ANczH",
    "outputId": "4ca89679-d717-44cd-f73c-f11765efa0c1"
   },
   "outputs": [],
   "source": [
    "print('Accuracy: ', np.round(accuracy_score(yhat, y_test),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tpme3Zbj0MYo"
   },
   "source": [
    "**Sanity Check**\n",
    "\n",
    "Here we use a function from the sklearn library, one of the most widely used in machine learning. MultinomialNB() implements the required algorithm, so the result of your implementation should be equal to the output of the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PFHDHyaQ2iGV",
    "outputId": "b2e62516-f355-4904-aa2e-7f0d870bfaf2"
   },
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes\n",
    "clf = naive_bayes.MultinomialNB(alpha=0.1)\n",
    "clf.fit(X_train,y_train)\n",
    "sk_y = clf.predict(X_test)\n",
    "print('Accuracy: ', np.round(accuracy_score(sk_y, y_test),3))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FDS_Exercise2_Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
